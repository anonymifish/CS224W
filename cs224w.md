# cs224w Machine Learning with Graphs

Winter 2021

## 1 Introduction

### Why graphs



### Applications of Graph ML



### Choice of Graph Representation



## 2 Traditional Methods for ML on Graphs



## 3 Node Embeddings

### Graph Representation Learning

传统的基于图的机器学期的过程，首先是通过输入的图得到图的结构特征（见上一节课），然后将这些特征送到传统的机器学习模型中完成下游任务。图表征学习（Graph Representation Learning）希望能够将特征工程（Feature Engineering）的部分省去，自动化的完成对特征的学习，之后完成下游任务。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230401133344730.png" alt="image-20230401133344730" style="zoom:25%;" />

一般的，图表征学习的目的是学习一个函数 $f:u\rightarrow\R^d$，将一个图上的顶点 $u$ 映射到一个 $d$ 维的向量上，这个向量就是特征的表示，称为 embedding 。这样我们就可以得到和任务无关（task-independent）的特征表示，可以用于各种下游任务，而不必像特征工程一样针对不同的任务使用不同的特征提取算法。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230401133757388.png" alt="image-20230401133757388" style="zoom: 25%;" />

我们期望这样的编码有下面的一些特点：

- 两个点 embedding 的相似性反映了网络中两个点的相似性 
- 编码网络信息
- 可以适用于多种下游任务

下面是 Node Embedding 的一个小例子，将一个小型的网络（Zachary's Karate Club） embedding 到一个二维空间上。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230401134126628.png" alt="image-20230401134126628" style="zoom: 33%;" />

### Encoder and Decoder Framework

完成 Node Embedding 的基本结构是 Encoder-Decoder 结构。为了更简单的描述这种结构，我们假设编码的图 $G$ 的顶点和边上没有附加的信息和特征。

我们希望在编码空间的相似性和在图上的相似性是有相关关系的。
$$
{\rm similarity}(u,v) \approx {\bf z}_v^T{\bf z}_u
$$
一般来说，编码空间的相似度一般用内积定义；从顶点到向量的映射函数（${\rm ENC}(·)={\bf z}_·$），以及网络中两个节点的相似度（${\rm similarity}(·,·)$）则需要自己定义。

下面我们给出 Encoder-Decoder 结构的关键概念：

- **Encoder** 将节点映射到向量空间，向量空间维数的选择和图的大小、特征有关
- 节点相似度函数（node similarity function）度量网络中两个节点的相似度
- **Decoder** 定义了向量空间两个向量的相似度，一般定义为内积
- 在完成定义之后不断的优化 Encoder 的参数使得 ${\rm similarity}(u,v) \approx {\bf z}_v^T{\bf z}_u$

最简单的 Encoder 实现是 Shallow Encoding，Encoder 做的事情是查表（embedding lookup）。
$$
{\rm ENC}(v)={\bf Z}·v={\bf z}_v
$$
其中 ${\bf Z}\in \R^{d\times \vert{\mathcal V}\vert}$，每一列是一个 $d$ 维的 embedding，一共有节点数列；$v\in \mathbb{I}^{\vert {\mathcal V}\vert}$，是一个指示向量，表示要选择哪一列。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230401140935409.png" alt="image-20230401140935409" style="zoom:25%;" />

这样我们的优化目标就是矩阵 ${\bf Z}$ 。关键的问题在于如何定义相似度以及如何优化矩阵，之后的内容将会关注这一问题。

有两点应该注意：

- 这是一个自监督/无监督（unsupervised/self-supervised）的过程，没有用到节点的标签和信息
- embedding 和任务无关

### Random Walk Approach

#### Notation

在讨论什么是 Random Walk 之前首先引入几个记号：

- Vector ${\bf z}_u$：顶点 $u$ 的编码
- Probability $P(v\vert {\bf z}_u)$：从顶点 $u$ 出发经过 random walk 经过顶点 $v$ 的（predicted） probability

在之后的关于概率的计算中，还会用到下面两个用于产生预测的概率的非线性函数：

- **Softmax**：$\sigma(z)_i=\cfrac{e^{z_i}}{\sum_{j=1}^Ke^{e_j}}$，将 $z\in\R^K$ 映射到 $K$ 个概率，且这 $K$ 个概率和为 1 。可以看到值越大映射到的概率越大，可以看成比较 soft 的 max 函数。
- **Sigmoid**：$S(x)=\cfrac{1}{1+e^{-x}}$，S 形函数，将值映射到 $(0,1)$，并且在 $x=0$ 的时候值为 $0.5$ 。

#### Basic Idea

一次 random walk 的过程是从某个顶点出发，按照一定的策略 $R$ 走到下一个顶点，再按照策略走到下一个顶点……走的长度为 random walk 的 length 。

Random Walk 实际上提出了一种度量网络中节点相似度的方法——顶点 $v$ 出现在顶点 $u$ 出发的 random walk 路径上的可能性。在策略 $R$ 下，这一可能性记为 $P_R(v\vert u)$ 。优化的目标是使得 ${\bf z}_u^T{\bf z}_v\approx\theta \propto P_R(v\vert u)$ 。这样的相似度定义有如下两点好处：

- Expressivity：灵活的概率定义兼顾了局部（local）特征和高阶（higher-order）信息
- Efficiency：不需要考虑所有的顶点对，只需要考虑在一次 random walk 中出现的顶点对即可

我们将这个想法形式化一下，定义一个和顶点相关的集合 nearby nodes（neighborhood） $N_R(u)$ 。这个集合是从顶点 $u$ 出发，使用策略 $R$ random walk 经过的顶点的集合。这样我们要优化的目标函数为
$$
\max_f\sum_{u\in V}\log P(N_R(u)\vert {\bf z}_u)
$$
Random Walk 的基本步骤为：

1. 经过从 $u$ 开始的，使用策略 $R$ 的 short fixed-length random walks
2. 对于每个顶点 $u$ 获得重集（multiset） $N_R(u)$（因为在一次 random walk 中一个顶点可以被访问多次）
3. 根据给定 $u$ ，预测它的邻居 $N_R(u)$ 优化 embeddings（也就是上面的目标函数）

#### Random Walk Optimization

我们将目标函数改写一下
$$
{\mathcal L}=-\sum_{u\in V}\log P(N_R(u)\vert {\bf z}_u)=\sum_{u \in V}\sum_{v\in N_R(u)}-\log(P(v\vert{\bf z}_u))
$$
我们使用 Softmax 定义 $P(v\vert{\bf z}_u)$
$$
P(v\vert {\bf z}_u)=\cfrac{\exp({\bf z}_u^T{\bf z}_v)}{\sum_{n\in V}\exp({\bf z}_u^T{\bf z}_n)}
$$
组合在一起我们得到下面的式子

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230401160931278.png" alt="image-20230401160931278" style="zoom: 25%;" />

优化的目标是找到 ${\bf z}_u$ 最小化 $\mathcal L$ 。

然而这个式子计算的代价太大了，特别是当图非常巨大的时候。主要的唯一在于 Softmax 分母的归一化需要图中所有的节点参与计算。解决的方法是负采样（Negation Sampling）。
$$
\log(\cfrac{\exp({\bf z}_u^T{\bf z}_v)}{\sum_{n\in V}\exp({\bf z}_u^T{\bf z}_n)})\approx \log\big(\sigma({\bf z}_u^T{\bf z}_v)\big)-\sum_{i=1}^k\log\big(\sigma({\bf z}_u^T{\bf z}_{n_{i}})\big),n_i\sim P_V
$$
实际上，我们的目标函数改变了。负采样是一种形式的 Noise Contrastive Estimation（NCE），这是上述不等式成立的依据。我们选取 $k$ 个点作为 negation samples，这些点服从分布 $P_V$ 。一种常见的选取点的方式是根据 degree 的分布。

- 更多的 $k$ 使得估计更加的准确
- 更多的 $k$ 使得估计有更多的对背景噪声的偏移
- 一般选择 5-20 个点

优化目标函数使用的方法是统计梯度下降（Stochastic Gradient Descent）：

1. 初始化 $z_i$ 为随机值
2. 不断的迭代直到收敛：$\mathcal L^{(u)}=\sum_{v\in N_R(u)} -\log(P(v\vert {\bf z}_u))$
   - 采样一个点 $i$，对于所有的 $j$ 计算 $\cfrac{\part \mathcal L^{(i)}}{\part z_j}$
   - 对于所有的 $j$ ，$z_j \leftarrow z_j-\eta\cfrac{\part\mathcal L^{(i)}}{\part z_j}$，$\eta$ 称为学习率（learning rate）

#### node2vec

上面的内容讨论的都是如何进行优化。本节我们讨论另一个关键问题：如何定义相似性（random walk 的策略）。

最简单的做法是进行固定长度，无偏（unbiased）的 random walk，这样的策略称为 [DeepWalk from Perozzi et al., 2013](https://arxiv.org/abs/1403.6652) 。但是这对于相似性的约束太强了，因此我们使用更一般的策略 node2vec 。

node2vec 用灵活的、有偏的 random walk 将局部的（local）和全局的（global）信息结合起来。实际上就是对 BFS 和 DFS 的结合。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403174148339.png" alt="image-20230403174148339" style="zoom:33%;" />

对于长度为 3 的路径（集合有 3 个元素），我们可以定义两种 neighborhood $N_R(u)$ 

- $N_{BFS}(u)=\{s_1,s_2,s_3\}$，局部微观的视角
- $N_{DFS}(u)=\{s_4,s_5,s_6\}$，全局宏观的视角

node2vec 是一种有偏定长二阶的 random walk 方法（biased fixed-length 2nd-order random walk）。为了将这两种策略结合起来，我们引入两个参数。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230404133809638.png" alt="image-20230404133809638" style="zoom:33%;" />

- Return parameter $p$：是否回到上一个节点

- In-out parameter $q$：选择是遵循 DFS 策略还是 BFS 策略，可以理解为 BFS 的DFS 的比例

  <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230404134011741.png" alt="image-20230404134011741" style="zoom:33%;" />

  如果 $p$ 的值小，更倾向于 BFS；如果 $q$ 的值小，更倾向于 DFS 。

  关于 random walk 还有其他的算法改进，node2vec 只是其中的一种。

### Embedding Entire Graphs

我们希望将整张图或者子图 $G$ 编码为 ${\bf z}_G$ 。这样我们可以对整张图进行分类。

#### Approach 1 Embed Nodes and Sum/Avg

最简单的想法是将所有节点的 embedding 求和（或者求均值）作为（子）图的 embedding 。
$$
{\bf z}_G=\sum_{v\in G}{\bf z}_v
$$

#### Approach 2 Create Super Node and Embed

第二种想法是加入一个虚节点（virtual node），用它的 embedding 表示整张图。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230404135811133.png" alt="image-20230404135811133" style="zoom:33%;" />

#### Approach 3 Anonymous Walk Embeddings

Anonymous Walk 的路径上的记录不再是记录节点的编号，而是记录节点第一次被访问的时间戳。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230404140444962.png" alt="image-20230404140444962" style="zoom: 50%;" />

Anonymous Walk 不再关注具体经过的哪个节点，而是关注节点访问的顺序，从而提取出图的结构。两个经过节点完全不同的 random walk 可能对应相同的 Anonymous Walk 记录。

比如说长度为 3 的 anonymous walk 有下面 5 中可能：`111`,`112`,`121`,`122`,`123` 。不同长度的 anonymous walk 对应的不同路径数量如下所示。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230404140813190.png" alt="image-20230404140813190" style="zoom: 50%;" />

对于长度为 $l$ 的 anonymous walk，它的可能路径有 $\{w_i\vert i=1,2,...,l\}$ 。我们可以用路径的概率分布来编码整张图：${\bf z}_G\in \R^{l}$，${\bf z}_G[i]$ 为路径 $w_i$ 在图 $G$ 中进行 anonymous walk 出现的概率。



## 4 Graph as Matrix

上一节中我们讨论了如何对节点进行编码，详细介绍了一种重要的算法 random walk 。本节将进一步的拓展讨论 random walk 算法，用这种算法解决 PageRank 问题，我们从矩阵的视角重新审视 random walk 算法，感受算法和矩阵的内在联系。

### PageRank

我们将网络看成是一个图，节点是网页，边是超链接。我们讨论的网页连接是导航的（navigational）链接而不是现在常见的事务链接。除了网页，论文、百科的引用关系也是类型的图结构，它们都可以被抽象成有向图。图的节点的重要程度是不相同的，PageRank 要解决的问题就是将这些节点关于重要性排序。下面所讲几种方法称为 Link Analysis approaches 。

#### Flow Model

一个想法是将边看成是投票（vote）关系，如果网页有更多的入边，那么这个网页更重要。每个入边的重要程度也因为源节点的重要程度而不同，来自重要网页的链接更加重要。我们将这个想法形式化一下，如果网页 $i$ 的重要程度是 $r_i$ ，有 $d_i$ 条出边（出度），那么每条边的重要程度是 $\cfrac{r_i}{d_i}$；那么节点 $j$ 的 rank $r_j$ 表示为
$$
r_j=\sum_{i\rightarrow j}\cfrac{r_i}{d_i}
$$
这个模型称之为 Flow Model ，对于一张图，有 $\vert \mathcal V\vert$ 个 flow equations，解决 PageRank 问题就变成了解线性方程组。显然选择高斯消元法解方程组的复杂度太高了，我们从线性代数的视角来重写这个方程组。

我们引入概率邻接矩阵（Stochastic adjacency matrix） ${\bf M}$ 。一般的邻接矩阵 $M_{ij} = 1$ 如果有边 $i\rightarrow j$ 。而概率邻接矩阵的定义为 ${\bf M}_{ij}=\cfrac{1}{d_j}$ ，如果有边 $j\rightarrow i$ 。矩阵 ${\bf M}$ 的每一列之和为 1 ；定义 rank vector ${\bf r}$ ，每一项 $r_i$ 是第 $i$ 个页面的重要程度，$\sum_ir_i=1$ 。这样 flow equations 写成 ${\bf r}={\bf M}·{\bf r}$，即
$$
\begin{bmatrix}
r_1\\
r_2\\
r_3\\
.\\
.\\
.\\
r_n
\end{bmatrix}
=
\begin{bmatrix}
M_{11} & M_{12} & ... & M_{1n}\\
M_{21} & M_{22} & ... & M_{2n}\\
M_{21} & M_{22} & ... & M_{2n}\\
.      & .      & ... & .\\
.      & .      & ... & .\\
.      & .      & ... & .\\
M_{21} & M_{22} & ... &M_{2n}\\
\end{bmatrix}
·
\begin{bmatrix}
r_1\\
r_2\\
r_3\\
.\\
.\\
.\\
r_n
\end{bmatrix}
=
\begin{bmatrix}
\sum_{i\rightarrow 1\frac{r_i}{d_i}}\\
\sum_{i\rightarrow 2\frac{r_i}{d_i}}\\
\sum_{i\rightarrow 3\frac{r_i}{d_i}}\\
.\\
.\\
.\\
\sum_{i\rightarrow n\frac{r_i}{d_i}}\\
\end{bmatrix}
$$

#### Random Walk

其实这个模型和 random walk 模型是一致的。想象一个浏览网页的人，在时间 $t$ 在页面 $i$ ，在 $t+1$ 的时候这个人会随机选择点击一个超链接跳转到另一个网页。定义 ${\bf p}(t)$ 的第 $i$ 个分量是在 $t$ 时刻这个人在网页 $i$ 的概率。那么 ${\bf p}(t)$ 就是一个概率分布。我们将它形式化一下，如果一个网页 $j$ 有多个网页指向它，那么下一个时刻访问到它的概率为
$$
r_j=\sum_{i\rightarrow j}\cfrac{r_i}{d_{out}(i)}
$$
借用刚才定义的概率邻接矩阵，我们有 ${\bf p}(t+1)={\bf M}·{\bf p}(t)$ 。如果这个随机访问的人 random walk 的时间足够长，那么 ${\bf p}(t+1)={\bf M}·{\bf p}(t)={\bf p}(t)$ ，那么 ${\bf p}(t)$ 是一个稳定的分布。看看刚才定义的“网页的重要程度” ${\bf r}={\bf M}·{\bf r}$，我们发现其实重要程度 ${\bf r}$ 实际上就是 random walk 的稳定概率分布！

再结合线性代数的知识，$1·{\bf r}={\bf M}·{\bf r}$，所以 rank vector ${\bf r}$ 就是矩阵 ${\bf M}$ 特征值 $\lambda=1$ 的一个特征向量。所以 ${\bf r}$ 就是 ${\bf M}({\bf M}({\bf M}...({\bf M} {\bf u})))$ 的极限，我们可以用 power iteration 的方法求解出 ${\bf r}$ 。

#### Solving PageRank

有了上面的讨论，我们可以写出 power iteration 算法。

1. 初始化 ${\bf r}^0 = [\cfrac{1}{N}, \cfrac{1}{N},..., \cfrac{1}{N}]^T$
2. 迭代 ${\bf r}^{(t+1)=}={\bf M}·{\bf r^{t}}$
3. 当 $\vert {\bf r}^{(t+1)}-{\bf r}^t\vert_1<\epsilon$ 的时候停止迭代

这里我们使用了一范数（$L_1$ norm），$\vert x\vert_1=\sum_{i=1}^N\vert x_i\vert$。一般来说迭代 50 次左右即可收敛。

然而由于图的结构，存在下面两个问题：

- dead ends（have no out-links）：如果一个点没有出边，那么这个点的分量会在下一次迭代时消失，称为 leak out 。

  <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230402143105998.png" alt="image-20230402143105998" style="zoom:25%;" />

- spider traps（all out-links within the group）：会使得这个子图最终吸收所有的重要性，其他节点的重要性为 0 。

  <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230402143049786.png" alt="image-20230402143049786" style="zoom:25%;" />

显然，这两种情况虽然收敛，但是收敛到的结果并不是我们想要的。

为了解决 spider traps ，我们选在引入参数 $\beta$，在每一步选择的时候，以 $\beta$ 的可能性 random walk，以 $1-\beta$ 的可能性跳转到随机的页面上。一般会选取 $\beta\in[0.8,0.9]$ 。这样就会从这个封闭的子图中跳转出来。实际上 spider traps 不是一个问题，只不过收敛到了我们不想要的结果。

为了解决 dead ends，我们为没有出边的节点加入 teleport，随机的跳转到其他节点。dead ends 是一个问题，这是因为矩阵 $\bf M$ 不是概率邻接矩阵（一列之和不是 1），加入 teleports 之后我们将矩阵修复为了概率邻接矩阵。

Google 将这两种解决方案结合起来，如果有 dead ends 首先修复为概率邻接矩阵，在每一步跳转的时候引入参数 $\beta$。这样 PageRank equation 变为
$$
r_j=\sum_{i\rightarrow j}\beta\cfrac{r_i}{d_i}+(1-\beta)\cfrac{1}{N}
$$
我们可以构建 Google Matrix ${\bf G}$ ：${\bf G}=\beta {\bf M}+(1-\beta)\Big[\cfrac{1}{N}\Big]_{N\times N}$。要解的方程组是 ${\bf r}={\bf G} · {\bf r}$ ，可以使用 power iteration 求解。

下面是一个 PageRank 解决之后的例子，可以看到计算得到的重要程度很合理。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230402145314812.png" alt="image-20230402145314812" style="zoom: 33%;" />

### Personalized PageRank



### Matrix Factorization



## 5 Node Classification

给定一张图，图的节点有标签，在知道部分节点的标签的情况下我们希望推测出其他节点的标签，这就是本节讨论的问题。Node Embedding 给出了一种无监督学习进行节点“分类”的方法。而本节的方法是半监督（semi-supervised）的分类方法。

### Message Passing Framework

本节所讨论的分配标签的问题称为 collective classification 问题。后面介绍的三个方法有统一的 message passing 结构。也就是通过边在节点之间传递信息。

这个方法基于的基本假设是 homophily ，相似的节点（同一类的节点）之间会有边相连（correlations exists in networks），所以相邻的节点更可能有相同的标签。这个想法由两种社会学现象提出（基本假设）：

- Homophily：一个人会更倾向于和相似的人建立社会连接，比如有相同兴趣的人更可能有交往
- Influence：社会连接会影响一个人的特征，让有社会连接的两个人的特征更加相近。比如一个人的兴趣可能会让身边人也产生类似的兴趣。

我们认为一个节点的类别受下面几种因素影响：

- 节点本身的特征（Features）
- 节点邻居的标签（Labels）
- 节点邻居的特征

设 ${\bf A}$ 是一个 $n\times n$ 的邻接矩阵，${\bf Y}=\{0,1\}^n$ 是要预测的标签，如果节点 $v$ 是类别 1 的，那么 $Y_v=1$ ；如果节点 $v$ 是类别 0 的，那么 $Y_v=0$ 。

我们认为节点类别的概率满足马尔科夫假设（Markov Assumption）：节点 $v$ 的类别 $Y_v$ 依赖于节点的邻居 $N_v$，即 $P(Y_v)=P(Y_v\vert N_v)$ 。

一般而言 collective classification 的解决分成三步

1. Local classifier：设置初始的标签（不利用网络信息，基于节点特征）
2. Relational classifier：利用网络的结构收集邻居的信息
3. Collective inference：根据邻居的信息更新节点属于某类别的概率，再次传播信息直到收敛

### Relational Classification

Relational Classification 的基本想法是节点 $v$ 是某个类别的可能性 $Y_v$ 是它的邻居的可能性的平均。对于给出的已经标记的节点 $v$，我们将 $Y_v$ 初始化为 ground-truth $Y_v^*$，$P(Y_v=Y_v^*)=1$；对于没有标记的节点，初始化为 $P(Y_v=c)=0.5$ 。之后我们按照随机的顺序更新所有节点的概率，直到到达最大迭代次数或者所有节点的概率都收敛。节点 $v$ 属于类别 $c$ 的概率更新公式为
$$
P(Y_v=c)=\cfrac{1}{\sum_{(v,u)\in \mathcal E}A_{v,u}}\sum_{(v,u)\in\mathcal E}A_{v,u}P(Y_u=c)
$$
其中 $A_{v,u}$ 为边的权重，$P(Y_v=c)$ 为节点 $v$ 为类别 $c$ 的概率。

下面我们看一个例子。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403143836862.png" alt="image-20230403143836862" style="zoom: 33%;" />

第一次迭代，按照节点的大小顺序依次进行更新。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403143938761.png" alt="image-20230403143938761" style="zoom:33%;" />

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403144100661.png" alt="image-20230403144100661" style="zoom:33%;" />

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403144123909.png" alt="image-20230403144123909" style="zoom:33%;" />

在第一次迭代之后节点划分为类别 1 的概率如下。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403144148912.png" alt="image-20230403144148912" style="zoom:33%;" />

第二次迭代之后结果如下，因为节点 9 的概率和第一次迭代的结果相同，因此节点 9 收敛，之后的迭代不再更新。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403144356597.png" alt="image-20230403144356597" style="zoom:33%;" />

第三次迭代之后结果如下。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403144452049.png" alt="image-20230403144452049" style="zoom:33%;" />

第四次迭代之后结果如下。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403144530935.png" alt="image-20230403144530935" style="zoom:33%;" />

最后一轮迭代之后全部收敛，结果如下。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403144658193.png" alt="image-20230403144658193" style="zoom:33%;" />

这个算法存在的问题有：

- 算法可能不收敛
- 模型不能利用到节点的特征信息

### Iterative Classification

Iterative Classification 的想法是同时根据节点 $v$ 的属性（attributes）${\bf f}_v$ 和节点 $v$ 的邻居 $N_v$ 的标签 ${\bf z}_v$ 进行分类。

${\bf z}_v$ 的构建可以有多种选择：

- 邻居节点不同类别所占有的比例
- 邻居节点有最多节点数的类别
- 邻居节点的不同类别数量

Iterative Classification 的做法是训练两个分类器：

- $\phi_1({\bf f}_v)$ 根据节点的特征预测节点的类别
- $\phi_2({\bf f}_v,{\bf z}_v)$ 根据节点的特征和邻居节点的类别信息预测节点的类别

算法分成两个阶段：

1. 根据节点的属性预测节点的类别
   - 在训练集（training set）上训练分类器 $\phi_1({\bf f}_v)$ 和 $\phi_2({\bf f}_v,{\bf z}_v)$
2. 迭代修改节点的类型直到收敛
   - 在测试集（test set）上，根据 $\phi_1$ 首先给每个节点设置一个初始类别
   - 对每个节点 $v$ 不断重复直到收敛
     - 根据 $Y_u,u\in N_v$ 更新 ${\bf z}_v$
     - 根据 ${\bf z}_v$ 通过 $\phi_2$ 预测新的类别，更新 $Y_v$

算法不保证收敛。

下面来看一个例子，一个描述网页间引用关系的图，使用一个二维、取值为 0、1 的向量 ${\bf f}_v$ 描述网页的内容。我们可以使用一个分类器为未标注的节点分配一个类别。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403155634739.png" alt="image-20230403155634739" style="zoom:33%;" />

我们使用一个向量 ${\bf z}_v=[{\bf I},{\bf O}]$，${\bf I}=[{\bf I}_0,{\bf I}_1]$，${\bf O}=[{\bf O}_0,{\bf O}_1]$ 描述节点邻居的类别信息。${\bf I}_0=1$ 如果有一个类别为 0 的页面指向它；${\bf O}_0=1$ 如果它指向了一个类别为 0 的页面。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403161249863.png" alt="image-20230403161249863" style="zoom:33%;" />

1. 在一个不同的训练集上训练两个分类器，得到 $\phi_1$ 和 $\phi_2$ 。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403161347197.png" alt="image-20230403161347197" style="zoom:33%;" />

2. 在测试集上，利用特征和分类器 $\phi_1$ 为每个节点分配一个预测的类别 $Y_v$ 。

   <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403161824374.png" alt="image-20230403161824374" style="zoom:33%;" />

3. 迭代直到收敛。

   - 更新所有节点的 ${\bf z}_v$

     <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403161918230.png" alt="image-20230403161918230" style="zoom:33%;" />

   - 利用 $\phi_2$ 重新进行分类

     <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403162016687.png" alt="image-20230403162016687" style="zoom:33%;" />

   - 更新所有节点的 ${\bf z}_v$

     <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403162141274.png" alt="image-20230403162141274" style="zoom:33%;" />

   - 再次利用 $\phi_2$ 分类，发现收敛

     <img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403162237674.png" alt="image-20230403162237674" style="zoom:33%;" />

### Loopy Belief Propagation

上面介绍的两种方法都是在节点之间传递某种信息。Loopy Belief Propagation 就是基于传递信息的想法，不断让周围的顶点猜测某个顶点的类别，试图达成共识（consensus）。

首先看一个例子，建立对 message passing 的直观概念。我们假设有一个 path graph，我们希望统计图的节点总数量，每个节点只能和邻居传递消息。解决办法是定义节点的序关系，设边的方向（信息传递方向）是小编号到大编号，第 $i$ 个节点给第 $i+1$ 个节点传递到目前为止有几个节点，第 $i+1$ 个节点在加入自己的信息之后传递给下一个节点。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403163739712.png" alt="image-20230403163739712" style="zoom:33%;" />

实际上，对于树也可以方便的进行信息传递（比如从叶子到根传递信息）。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403163859364.png" alt="image-20230403163859364" style="zoom:33%;" />![image-20230403163915840](E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403163915840.png)<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403163859364.png" alt="image-20230403163859364" style="zoom:33%;" />![image-20230403163915840](E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403163915840.png)

下面我们描述 Loopy Belief Propagation 算法。我们定义几个记号。

- Label-label potential matrix $\Psi(Y_i,Y_j)$：这个值和如果节点 $j$ 的类别是 $Y_j$，它的邻居 $i$ 的类别是 $Y_i$ 的可能性有关
- Prior belief $\Phi$：$\Phi_i(Y_i)$ 和节点 $i$ 是类别 $Y_i$ 有关
- $m_{i\rightarrow j}(Y_j)$：节点 $i$ 传递的信息，估计节点 $j$ 是类别 $Y_j$ 的概率
- $\mathcal L$：类别的集合

算法流程如下：

1. 初始化所有的信息为 1

2. 所有节点不断计算发送给其他节点的信息
   $$
   m_{i\rightarrow j}(Y_j)=\sum_{Y_i\in\mathcal L}\Psi(Y_i,Y_j)\Phi_i(Y_i)\prod_{k\in N_i-\{j\}}m_{k\rightarrow i}(Y_i), \forall Y_j\in\mathcal L
   $$

3. 不断迭代发送信息直到收敛

$\prod_{k\in N_i-\{j\}}m_{k\rightarrow i}(Y_i)$ 是节点 $i$ 的邻居发送给它的信息，表示了其他的节点认为节点 $i$ 的类别是 $Y_i$ 的可能性有多大，乘上 $\Phi_i(Y_i)$ 表示节点 $i$ 类别为 $Y_i$ 的可能性。乘上 $\Psi(Y_i,Y_j)$ 就是认为在节点 $i$ 类别为 $Y_i$ 的情况下节点 $j$ 类别为 $Y_j$ 的可能性。将所有的节点 $i$ 的可能类别带来的节点 $j$ 类别为 $Y_j$ 的可能性相加就是 $i$ 认为 $j$ 类别为 $Y_j$ 的总可能性。

设 $b_i(Y_i)$ 是节点 $i$ 类别为 $Y_i$ 的可能性，在算法收敛的时候有
$$
b_i(Y_i)=\Phi_i(Y_i)\prod_{j\in N_i}m_{j\rightarrow i}(Y_i), \forall Y_i \in \mathcal L
$$
这个算法的主要问题在于无法处理环。环上无法定义序关系，消息之间相互依赖（上面的乘法基于消息之间独立），可能使得有偏差的消息不断的加强，最终无法收敛。

<img src="E:\Study\Graph_ML\cs224w\cs224w.assets\image-20230403170949037.png" alt="image-20230403170949037" style="zoom:33%;" />

虽然有环的问题，但是实际的图还是树类型的图更多；即便有环，环的长度也很长，有很多分支，这仍是一个很好的算法。

函数 $\Psi$ 通过训练得到，可以简单的拓展到高维，实现更复杂的标签分类。同样，算法不保证收敛。

## 6 Graph Neural Networks

### Deep Graph Encoders

回顾 Node Embedding 的内容，我们学习了映射函数 $f$ 将图的节点映射到一个 $d$ 维向量。在定义了相似度函数 ${\rm similarity(·)}$ 和 encoder 之后就可以优化 encoder 参数，完成编码。这一过程有很多不足。

- 对于 Shallow Encoder，参数量是 $O(\vert V\vert)$ 的，这意味着在图非常大的时候参数量很大，成本高，效率低
- 对于在训练的时候不存在（或者不可见）的节点，无法得到该节点的 embedding
- 无法利用节点的特征

因此我们使用基于图神经网络（Graph Neural Networks，GNN）的深度方法编码一个节点。现在的 encoder ${\rm ENC}(v)$ 是一个基于图结构的多层的（multiple layers）非线性（non-linear）的变换。同样的，所有这样的 encoder 可以和相似度函数结合，使得 embedding 的相似和图中节点的相似度相关。

传统的机器学习方法面对的是结构化、简单、有序的数据，而图结构不然。

- 图是任意大小的，且有复杂的拓扑结构，没有规整的空间局部性（spatial locality）
- 没有固定的节点的顺序
- 动态变化，节点有多模态的特征

### Basic of Deep Learning

对于监督学习，一般的形式是输入为 ${\bf x}$，目标是预测标签 ${\bf y}$ 。输入可以是实数向量、序列（比如自然语言）、矩阵（比如图像）、图……

机器学习可以看成是一个优化问题。我们将这个优化问题形式化为
$$
\min_\Theta\mathcal L({\bf y},f({\bf x}))
$$

- $\Theta$ 是我们需要优化的一组参数，可以是一个标量、向量、矩阵……比如对于 Shallow Encoder 
- $\mathcal L$

### Deep Learning for Graphs



### Graph Convolutional Networks and GraphSAGE


